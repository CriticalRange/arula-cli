# AI-ENHANCED by ARULA
# Last updated: 2025-12-23

PROJECT_MANIFEST v1.0

# METADATA
name: arula
type: Rust Workspace
language: Rust

# WORKFLOW
build: cargo build -p arula_cli
test: cargo test -p arula_core
run: cargo run -p arula_cli
lint: cargo clippy && cargo fmt
check: cargo check

# ESSENCE
ARULA is a sophisticated autonomous AI command-line interface built with Rust, featuring a modern terminal UI (TUI) with full-duplex communication, multi-provider AI support, and a powerful tool-calling framework. The project consists of three main crates: arula_core (shared business logic), arula_cli (terminal interface), and arula_desktop (desktop GUI wrapper). The system enables seamless AI-human collaboration through an interactive chat interface that supports streaming responses, tool execution, and conversation persistence.

## Key Capabilities
- Full-duplex terminal interaction using ratatui with async input handling
- Multi-provider AI support (OpenAI, Anthropic, Ollama, OpenRouter, Z.AI, custom endpoints)
- Type-safe tool calling framework with automatic execution and result streaming
- Model Context Protocol (MCP) client for dynamic tool discovery and loading
- Vision/screenshot capabilities via the Visioneer tool
- Native terminal scrollback (no alternate screen) for seamless workflow integration
- Robust configuration management with YAML/JSON support and migration system
- Conversation persistence with automatic saving and loading
- Git state tracking for branch restoration across tool executions

# ARCHITECTURE

## Workspace Structure
```
arula/
├── arula_core/          # Shared business logic and state management
│   ├── src/
│   │   ├── app.rs           # Central App orchestrator (85KB, 2090 lines)
│   │   ├── api/             # AI provider clients and agent framework
│   │   │   ├── agent.rs          # Type-safe tool calling framework
│   │   │   ├── agent_client.rs   # High-level agent client (31KB)
│   │   │   ├── api.rs            # Legacy streaming client (101KB)
│   │   │   ├── stream.rs         # Unified streaming logic (38KB)
│   │   │   ├── models.rs         # Model caching system
│   │   │   └── http_client.rs    # Optimized HTTP with pooling
│   │   ├── tools/          # Built-in and MCP tools
│   │   │   ├── builtin/          # Modular built-in tools
│   │   │   ├── visioneer.rs      # Vision/screenshot (50KB)
│   │   │   ├── mcp.rs            # MCP client
│   │   │   └── mcp_dynamic.rs    # Dynamic MCP loading
│   │   ├── utils/          # Shared utilities
│   │   │   ├── config.rs         # Configuration management (41KB)
│   │   │   ├── conversation.rs   # Conversation persistence
│   │   │   ├── chat.rs           # Message types
│   │   │   ├── git_state.rs      # Git branch tracking
│   │   │   ├── debug.rs          # Debug logging
│   │   │   └── changelog.rs      # Version history
│   │   └── session_manager.rs    # Multi-conversation management
├── arula_cli/           # Terminal interface
│   ├── src/main.rs          # Entry point with CLI parsing
│   └── src/ui/              # TUI implementation
│       ├── tui_app.rs           # Main TUI app (42KB)
│       ├── input_handler.rs     # Async input handling (46KB)
│       ├── custom_spinner.rs    # Orbital animations (15KB)
│       ├── response_display.rs  # AI response rendering
│       ├── menus/               # Interactive menu system
│       └── output/              # Output formatting
└── arula_desktop/       # Desktop GUI wrapper (WIP)
```

## Core Architecture Patterns

### 1. Dual AI System
The project maintains two parallel AI client architectures:
- **Legacy API** (`api.rs`): Traditional streaming via raw HTTP requests for backward compatibility
- **Modern Agent** (`agent.rs` + `agent_client.rs`): Type-safe tool calling with automatic execution
- Both use `tokio::sync::mpsc::unbounded_channel()` for non-blocking response streaming
- The `App` struct coordinates between both systems via the `AiResponse` enum

### 2. Full-Duplex Terminal I/O
- Uses `ratatui` with custom `input_handler.rs` for async keyboard input
- AI responses stream to output while user continues typing
- Achieved through separate channels: one for user input, one for AI streaming
- No alternate screen - all output flows to native terminal scrollback buffer

### 3. Tool Calling Framework
```rust
// Modern tool pattern
#[derive(Debug, Deserialize)]
pub struct BashTool {
    pub command: String,
}

impl Tool for BashTool {
    fn name(&self) -> &str { "execute_bash" }
    async fn execute(&self) -> Result<Value> { /* ... */ }
}
```

### 4. State Management
- Central `App` struct in `arula_core` holds all application state
- Shared state via `Arc<Mutex<T>>` for concurrent access
- Cancellation tokens for graceful request abortion
- Separate channels for AI streaming vs. user input

### 5. Configuration System
- Multi-provider support in `config.rs` with YAML/JSON formats
- Automatic migration between config versions
- Provider-specific settings (API keys, base URLs, model names)
- Per-conversation settings for fine-grained control

## Key Design Decisions

### Why Rust 2024 Edition?
- Modern syntax improvements and better error handling
- Enhanced async/await performance
- Future-proof foundation for long-term project

### Why Workspace Structure?
- Clear separation: core logic vs. UI vs. platform-specific code
- Shared core (`arula_core`) can be reused across CLI, desktop, mobile
- Independent dependency management per crate

### Why Dual AI Architecture?
- Legacy API provides fallback compatibility
- Modern agent framework enables advanced tool calling
- Gradual migration path from old to new system
- Supports both simple streaming and complex tool workflows

### Why MCP (Model Context Protocol)?
- Industry-standard for tool discovery and dynamic loading
- Enables third-party tool integration without code changes
- Standardized tool interface across AI providers
- Future-proofs the system for evolving AI ecosystem

# GOTCHAS & PITFALLS

## Critical Gotchas

### 1. Async/Await Context
- The TUI runs in a `tokio::runtime`, so all async operations must use `.await`
- Never block the main event loop with synchronous I/O
- Use `tokio::task::spawn_blocking()` for CPU-intensive or blocking operations

### 2. Tool Execution Order
- Tool calls execute asynchronously and may complete out-of-order
- Track tool execution via `tool_call_id` in responses
- Use `pending_tool_results` queue to batch results before sending to AI

### 3. Git State Restoration
- Tools that modify git state (branch switches, commits) trigger auto-restore
- Original git branch is restored when conversation ends
- Disable with `git_state_tracker.disabled = true` if needed

### 4. MCP Tool Loading
- MCP tools load asynchronously on startup via `initialize_tool_registry()`
- First run may be slow due to tool discovery and caching
- Cache invalidates when MCP server configuration changes

### 5. Configuration Migration
- Config format changes between versions - always run migration on load
- Old configs auto-migrate to new format in `Config::load_or_default()`
- Manual edits to config YAML should respect current schema

## Common Pitfalls

### 1. Forgetting to Initialize Components
```rust
// WRONG - will panic when accessing agent_client
let app = App::new()?;
app.send_to_ai("hello").await?;

// RIGHT - initialize in correct order
let mut app = App::new()?;
app.initialize_tool_registry().await?;
app.initialize_agent_client();
app.send_to_ai("hello").await?;
```

### 2. Blocking the Event Loop
```rust
// WRONG - blocks UI during tool execution
let result = std::fs::read_to_string(path)?;

// RIGHT - use async or spawn_blocking
let result = tokio::task::spawn_blocking(move || {
    std::fs::read_to_string(path)
}).await?;
```

### 3. Not Handling Stream End
```rust
// WRONG - stream never completes, memory leak
while let Some(event) = stream.next().await {
    match event {
        StreamEvent::Text(text) => print!("{}", text),
        // Missing StreamEvent::End handling
    }
}

// RIGHT - handle stream termination
while let Some(event) = stream.next().await {
    match event {
        StreamEvent::Text(text) => print!("{}", text),
        StreamEvent::End => break,
    }
}
```

### 4. Ignoring Cancellation
```rust
// WRONG - can't abort in-flight requests
app.send_to_ai(long_prompt).await?;

// RIGHT - support cancellation via token
app.send_to_ai_with_cancellation(long_prompt, token).await?;
// Later: token.cancel();
```

## Conventions

### Naming
- Module names: `snake_case` (e.g., `agent_client.rs`)
- Type names: `PascalCase` (e.g., `AgentClient`, `ChatMessage`)
- Private helpers: prefix with `_` or `internal_`
- Constants: `SCREAMING_SNAKE_CASE`

### Error Handling
- Use `anyhow::Result<T>` for application errors
- Use `thiserror` for custom error types in public APIs
- Always provide context with `.context()` or `.with_context()`
- Log errors at appropriate level (debug, info, warn, error)

### Documentation
- Document public APIs with rustdoc comments (`///`)
- Use module-level comments (`//!`) to explain purpose
- Include example code in documentation
- Document async function requirements (runtime, cancellation)

### Testing
- Unit tests in same file as code (mod tests module)
- Integration tests in `tests/` directory
- Use `tokio::test` for async tests
- Mock external dependencies (API clients, file system)

# DEVELOPMENT TASKS

## Adding a New Tool

1. Create tool struct in `arula_core/src/tools/builtin/`:
```rust
use serde::Deserialize;
use crate::api::agent::Tool;

#[derive(Debug, Deserialize)]
pub struct MyTool {
    pub param: String,
}

impl Tool for MyTool {
    fn name(&self) -> &str { "my_tool" }
    fn description(&self) -> &str { "Does something cool" }
    
    async fn execute(&self) -> anyhow::Result<serde_json::Value> {
        Ok(serde_json::json!({"result": "done"}))
    }
}
```

2. Register in `arula_core/src/tools/mod.rs`

3. Add to tool registry initialization in `app.rs`

## Adding a New AI Provider

1. Add provider variant to `Provider` enum in `config.rs`
2. Implement provider-specific client in `api.rs` or `agent_client.rs`
3. Add provider configuration options to config schema
4. Update `models.rs` for model caching support
5. Add provider to init function in `main.rs`

## Debugging Issues

1. Enable debug mode:
```bash
cargo run -- --debug
```

2. Check debug logs:
- Debug output shows AI interactions, tool calls, timing
- Set `ARULA_DEBUG=1` environment variable
- Use `RUST_LOG=debug` for detailed tracing

3. Common issues:
- **Tool not found**: Check tool registry initialization
- **API timeout**: Increase timeout in provider config
- **Config errors**: Validate YAML syntax, check migration
- **MCP connection failures**: Verify MCP server URL and authentication

## Testing Changes

1. Run full test suite:
```bash
cargo test --workspace
```

2. Run specific crate tests:
```bash
cargo test -p arula_core
cargo test -p arula_cli
```

3. Run with clippy for linting:
```bash
cargo clippy --workspace -- -D warnings
```

4. Format code:
```bash
cargo fmt --all
```

## Building for Release

1. Optimized build:
```bash
cargo build --release -p arula_cli
```

2. Binary location:
```bash
./target/release/arula_cli
```

3. Size optimization:
- Use `lto = true` in Cargo.toml
- Strip symbols: `strip target/release/arula_cli`
- Consider `upx` for compression (Linux only)

## Configuration Management

Config location: `~/.config/arula/config.yml` (Linux), `~/Library/Application Support/arula/config.yml` (macOS), `%APPDATA%\arula\config.yml` (Windows)

Key config sections:
- `provider`: Default AI provider
- `providers`: Provider-specific settings (api_key, base_url, model)
- `mcp_servers`: MCP server configurations
- `conversation`: Auto-save settings
- `ui`: Theme and display options

# DEPENDENCIES

## Core Dependencies
- `tokio`: Async runtime (features: full)
- `serde`: Serialization (features: derive)
- `anyhow`: Error handling
- `rustyline`/`ratatui`: Terminal UI
- `crossterm`: Cross-platform terminal handling
- `console`: Colored output

## API Dependencies
- `reqwest`: HTTP client (features: json, streaming)
- `tokio-stream`: Stream utilities
- `futures`: Async utilities
- `uuid`: Unique identifiers (features: v4, serde)

## Tool Dependencies
- `walkdir`: Directory traversal
- `grep`: File searching
- `ignore`: .gitignore parsing

# ENVIRONMENT VARIABLES

- `ARULA_DEBUG`: Enable debug output (1 = enabled)
- `RUST_LOG`: Set log level (debug, info, warn, error)
- `ARULA_CONFIG_PATH`: Custom config file location
- `ARULA_NO_MCP`: Disable MCP tool loading
